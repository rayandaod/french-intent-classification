{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "from transformers import pipeline, FlaubertModel, FlaubertTokenizer\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "from src.helper import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "\n",
    "https://github.com/clinc/oos-eval#an-evaluation-dataset-for-intent-classification-and-out-of-scope-prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "clinc150_dataset = load_dataset(\"clinc_oos\", 'plus')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split = \"train\"\n",
    "\n",
    "# Get the data\n",
    "df = clinc150_dataset[split].to_pandas()\n",
    "\n",
    "# Get the class names and create a dictionary to map the class index to the class name\n",
    "labels = clinc150_dataset[split].features[\"intent\"].names\n",
    "labels = {i: name for i, name in enumerate(labels)}\n",
    "\n",
    "# Add a new column to the dataframe with the class name\n",
    "df[\"label\"] = df[\"intent\"].map(labels)\n",
    "\n",
    "# Drop the intent column\n",
    "df = df.drop(\"intent\", axis=1)\n",
    "\n",
    "# Print the first rows of the dataframe\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the distribution of text length\n",
    "df[\"text\"].str.len().hist(bins=30)\n",
    "plt.xlabel(\"Text length\")\n",
    "plt.ylabel(\"Number of examples\")\n",
    "plt.title(f\"Text length distribution of the {split} set\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the distribution of the classes\n",
    "plot_class_distribution(df, f\"Distribution of the classes in the {split} set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the class name to oos and intent as 'oos''s index for the examples not in the classes of interest\n",
    "df.loc[~df[\"label\"].isin(CLASSES_OF_INTEREST), \"label\"] = \"oos\"\n",
    "\n",
    "# Plot the distribution of the classes\n",
    "plot_class_distribution(df, f\"Distribution of the classes in the {split} set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downsample the oos class to have the same number of examples as the classes of interest\n",
    "df = pd.concat([\n",
    "    df[df[\"label\"] != \"oos\"],\n",
    "    df[df[\"label\"] == \"oos\"].sample(n=len(df[df[\"label\"] != \"oos\"]), replace=True, random_state=RANDOM_STATE)\n",
    "])\n",
    "\n",
    "# Plot the distribution of the classes\n",
    "plot_class_distribution(df, f\"Distribution of the classes in the {split} set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the dataframe as a csv file\n",
    "df.to_csv(f\"../data/clinc150_{split}_down.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model choice\n",
    "\n",
    "Evaluating which model is the best based on the given evaluation scores requires considering both the BLEU (Bilingual Evaluation Understudy) and chr-F scores across different test sets. The BLEU score is a metric for evaluating the quality of machine-translated text, with a higher BLEU score indicating better translation quality. The chr-F score is another evaluation metric that considers character n-gram precision and recall, with a higher chr-F score indicating better translation quality as well.\n",
    "\n",
    "Model 1: https://huggingface.co/Helsinki-NLP/opus-mt-tc-big-en-fr\n",
    "Has more diverse test sets, including both news and non-news domains.\n",
    "Generally achieves higher BLEU and chr-F scores across most of the test sets compared to Model 2.\n",
    "Demonstrates particularly strong performance on the multi30k_test_2017_mscoco test set.\n",
    "\n",
    "Model 2: https://huggingface.co/Helsinki-NLP/opus-mt-en-fr\n",
    "Has fewer test sets for evaluation and they are mainly focused on news domains.\n",
    "Achieves comparable BLEU and chr-F scores to Model 1 on the overlapping news domain test sets.\n",
    "Shows a strong performance on the Tatoeba.en.fr test set.\n",
    "\n",
    "Considerations\n",
    "Diversity of Test Sets: Model 1 has been evaluated on a wider variety of test sets, which could provide a more comprehensive understanding of its performance across different domains.\n",
    "Score Comparisons: On the overlapping test sets (news domain), the two models have comparable performance, with Model 1 having a slight edge in most cases.\n",
    "\n",
    "Domain Specificity: If the intended application of the model is in a specific domain (e.g., news translation), then the performance on the relevant test sets should be weighted more heavily.\n",
    "\n",
    "Conclusion\n",
    "Overall: Model 1 seems to be the better choice given its higher scores across a diverse set of test sets.\n",
    "Domain-Specific: If the application is focused on translating sentences similar to those in the Tatoeba test set, Model 2 might be the better choice as it has a higher BLEU and chr-F score on that specific test set.\n",
    "To make a more definitive conclusion, one could consider additional factors such as the model's efficiency, resource requirements, and any potential biases in the test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a translation pipeline\n",
    "translator = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-tc-big-en-fr\")\n",
    "\n",
    "# Create a new column for the translated text\n",
    "df[\"text_fr\"] = None\n",
    "\n",
    "# Reset index\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "# Translate the text to French and save it in the text_fr column\n",
    "# Use a for loop and tqdm to track the progress\n",
    "for i, row in tqdm(df.iterrows(), total=len(df)):    \n",
    "    df.loc[i, \"text_fr\"] = translator(df.at[i, \"text\"])[0][\"translation_text\"]\n",
    "\n",
    "# Print the first rows of the dataframe\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the dataframe as a csv file\n",
    "filename = f\"../data/clinc150_{split}_down_tr.csv\"\n",
    "# df.to_csv(filename, index=False)\n",
    "\n",
    "# Load the dataframe\n",
    "df = pd.read_csv(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df[\"label\"] == \"flight_status\"].sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embeddings\n",
    "\n",
    "We use FlauBERT to obtain embeddings from the translated sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelname = 'flaubert/flaubert_base_uncased' \n",
    "\n",
    "# Load pretrained model and tokenizer\n",
    "flaubert, log = FlaubertModel.from_pretrained(modelname, output_loading_info=True)\n",
    "flaubert_tokenizer = FlaubertTokenizer.from_pretrained(modelname, do_lowercase=True)\n",
    "# do_lowercase=False if using cased models, True if using uncased ones\n",
    "\n",
    "sentence = \"Le chat mange une pommmme.\"\n",
    "token_ids = torch.tensor([flaubert_tokenizer.encode(sentence)])\n",
    "\n",
    "last_layer = flaubert(token_ids)[0]  # [B, num_tokens, emb_dim]\n",
    "print(last_layer.shape)\n",
    "\n",
    "# print each token id and its corresponding token and make it readable\n",
    "print()\n",
    "for i, token_id in enumerate(token_ids[0]):\n",
    "    print(i, token_id.numpy(), '\\t', flaubert_tokenizer.decode(token_id))\n",
    "\n",
    "# The BERT [CLS] token correspond to the first hidden state of the last layer\n",
    "cls_embedding = last_layer[:, 0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add a new column to the dataframe with the list of embeddings\n",
    "df['embeddings'] = df['text'].apply(lambda x: flaubert(torch.tensor([flaubert_tokenizer.encode(x)]))[0].tolist()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# average the embeddings to avoid different magnitudes with different lengths\n",
    "df['embeddings_avg'] = df['embeddings'].apply(lambda x: [sum(i)/len(i) for i in zip(*x)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the dataframe as a csv\n",
    "df.to_csv(f'../data/clinc150_{split}_down_tr_emb.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "illuin_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
